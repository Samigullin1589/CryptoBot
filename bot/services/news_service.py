# =================================================================================
# –§–∞–π–ª: bot/services/news_service.py (–í–ï–†–°–ò–Ø "Distinguished Engineer" - –§–ò–ù–ê–õ–¨–ù–ê–Ø)
# –û–ø–∏—Å–∞–Ω–∏–µ: –û—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π, –ø–æ–ª–Ω–æ—Å—Ç—å—é
# –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å –∏–Ω—ä–µ–∫—Ü–∏–µ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.
# –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–µ—Ä–≤–∏—Å –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∞–Ω –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ __init__.
# =================================================================================
import asyncio
import logging
import hashlib
from typing import List, Optional

import aiohttp
import backoff
import feedparser
import redis.asyncio as redis
from aiogram import Bot

# –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–∏–ø—ã, –∞ –Ω–µ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
from bot.config.settings import Settings
from bot.utils.keys import KeyFactory
from bot.utils.models import NewsArticle
from bot.utils.text_utils import normalize_asic_name, sanitize_html

logger = logging.getLogger(__name__)

RETRYABLE_EXCEPTIONS = (aiohttp.ClientError, TimeoutError, asyncio.TimeoutError)

class NewsService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Å–±–æ—Ä–∞ –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä —Ç–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –∏–Ω—ä–µ–∫—Ü–∏—é.
    def __init__(
        self,
        redis: redis.Redis,
        http_session: aiohttp.ClientSession,
        settings: Settings,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ—Ä–≤–∏—Å –Ω–æ–≤–æ—Å—Ç–µ–π.

        :param redis: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç Redis.
        :param http_session: –ö–ª–∏–µ–Ω—Ç—Å–∫–∞—è —Å–µ—Å—Å–∏—è aiohttp.
        :param settings: –ì–ª–æ–±–∞–ª—å–Ω—ã–π –æ–±—ä–µ–∫—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫.
        """
        self.redis = redis
        self.session = http_session
        self.settings = settings
        self.keys = KeyFactory

    @backoff.on_exception(backoff.expo, RETRYABLE_EXCEPTIONS, max_tries=3)
    async def _fetch(self, url: str, headers: Optional[dict] = None) -> Optional[dict]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤—ã–π HTTP-–∑–∞–ø—Ä–æ—Å."""
        async with self.session.get(url, headers=headers, timeout=15) as response:
            response.raise_for_status()
            return await response.json()

    async def _fetch_from_cryptocompare(self) -> List[NewsArticle]:
        """–ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ API CryptoCompare."""
        api_key = self.settings.CRYPTOCOMPARE_API_KEY
        api_url = self.settings.endpoints.cryptocompare_news_api_url

        if not api_key or not api_url:
            logger.warning("CryptoCompare API key –∏–ª–∏ URL –Ω–µ –∑–∞–¥–∞–Ω—ã. –ü—Ä–æ–ø—É—Å–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.")
            return []

        headers = {'Authorization': f'Apikey {api_key.get_secret_value()}'}
        try:
            data = await self._fetch(str(api_url), headers=headers)
            if not data or "Data" not in data or not isinstance(data["Data"], list):
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –æ—Ç CryptoCompare.")
                return []
            
            return [
                NewsArticle(
                    title=article.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞'),
                    body=article.get('body', ''),
                    url=article.get('url', ''),
                    source='CryptoCompare',
                    timestamp=int(article.get('published_on', 0))
                ) for article in data["Data"]
            ]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –æ—Ç CryptoCompare: {e}")
            return []

    async def _fetch_from_rss(self, feed_url: str) -> List[NewsArticle]:
        """–ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –æ–¥–Ω–æ–π RSS-–ª–µ–Ω—Ç—ã."""
        try:
            async with self.session.get(feed_url, timeout=15) as response:
                if response.status != 200: return []
                content = await response.text()
            
            feed = await asyncio.to_thread(feedparser.parse, content)
            return [
                NewsArticle(
                    title=entry.title,
                    body=getattr(entry, 'summary', ''),
                    url=entry.link,
                    source=feed.feed.title,
                    timestamp=int(entry.get('published_parsed', (0,))[0])
                ) for entry in feed.entries
            ]
        except Exception as e:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å RSS-–ª–µ–Ω—Ç—É {feed_url}: {e}")
            return []

    async def get_latest_news(self) -> List[NewsArticle]:
        """–°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã."""
        logger.info("–ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π...")
        
        tasks = [self._fetch_from_cryptocompare()]
        all_feeds = self.settings.news_service.feeds.main_rss_feeds + self.settings.news_service.feeds.alpha_rss_feeds
        for url in all_feeds:
            tasks.append(self._fetch_from_rss(str(url)))

        results = await asyncio.gather(*tasks, return_exceptions=True)

        unique_articles = []
        seen_hashes = set()
        
        dedup_key = self.keys.news_deduplication_set()
        
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –Ω–æ–≤–æ—Å—Ç–µ–π: {result}")
                continue
            
            for article in result:
                title_hash = hashlib.sha1(normalize_asic_name(article.title).encode()).hexdigest()[:16]
                if title_hash in seen_hashes:
                    continue
                
                is_new = await self.redis.sadd(dedup_key, title_hash)
                if is_new:
                    unique_articles.append(article)
                    seen_hashes.add(title_hash)
        
        logger.info(f"–°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω. –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(unique_articles)}.")
        return sorted(unique_articles, key=lambda x: x.timestamp, reverse=True)

    async def send_news_digest(self, bot: Bot, chat_id: int):
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –¥–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π."""
        latest_news = await self.get_latest_news()
        if not latest_news:
            logger.info("–ù–µ—Ç –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏.")
            return

        news_to_send = latest_news[:self.settings.news_service.news_limit_per_source]
        
        message_parts = ["üì∞ <b>–°–≤–µ–∂–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π:</b>\n"]
        for article in news_to_send:
            message_parts.append(f"\n- <a href='{article.url}'>{sanitize_html(article.title)}</a> (<i>{sanitize_html(article.source)}</i>)")

        try:
            await bot.send_message(chat_id, "".join(message_parts), disable_web_page_preview=True)
        except Exception as e:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –¥–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —á–∞—Ç {chat_id}: {e}")
