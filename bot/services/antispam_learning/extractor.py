# bot/services/antispam_learning/extractor.py
"""
–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–ø–∞–º–∞.
"""
from typing import List, Set

from loguru import logger


class TextPhraseExtractor:
    """
    –ö–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç:
    - –û—Ç–¥–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ (—Ç–æ–∫–µ–Ω—ã)
    - –ë–∏–≥—Ä–∞–º–º—ã (–ø–∞—Ä—ã —Å–æ—Å–µ–¥–Ω–∏—Ö —Å–ª–æ–≤)
    - –¢—Ä–∏–≥—Ä–∞–º–º—ã (—Ç—Ä–æ–π–∫–∏ —Å–æ—Å–µ–¥–Ω–∏—Ö —Å–ª–æ–≤) - –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
    """
    
    # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    DEFAULT_MIN_TOKEN_LENGTH = 5
    DEFAULT_MAX_TOKENS = 50
    DEFAULT_MIN_PHRASE_LENGTH = 8
    DEFAULT_MAX_PHRASE_LENGTH = 64
    
    def __init__(
        self,
        min_token_length: int = DEFAULT_MIN_TOKEN_LENGTH,
        max_tokens: int = DEFAULT_MAX_TOKENS,
        min_phrase_length: int = DEFAULT_MIN_PHRASE_LENGTH,
        max_phrase_length: int = DEFAULT_MAX_PHRASE_LENGTH,
        use_trigrams: bool = False
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Ñ—Ä–∞–∑.
        
        Args:
            min_token_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∞
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            min_phrase_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–∑—ã
            max_phrase_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ñ—Ä–∞–∑—ã
            use_trigrams: –ò–∑–≤–ª–µ–∫–∞—Ç—å —Ç—Ä–∏–≥—Ä–∞–º–º—ã
        """
        self.min_token_length = min_token_length
        self.max_tokens = max_tokens
        self.min_phrase_length = min_phrase_length
        self.max_phrase_length = max_phrase_length
        self.use_trigrams = use_trigrams
        
        logger.debug(
            f"üîß TextPhraseExtractor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω "
            f"(min_token: {min_token_length}, max_tokens: {max_tokens}, "
            f"trigrams: {use_trigrams})"
        )
    
    def extract_phrases(self, text: str) -> Set[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            text: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (lowercase, –±–µ–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏)
            
        Returns:
            –ù–∞–±–æ—Ä —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑
        """
        if not text:
            return set()
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ç–æ–∫–µ–Ω—ã –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
        tokens = self._tokenize(text)
        
        if not tokens:
            return set()
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ñ—Ä–∞–∑—ã
        phrases = set()
        
        # 1. –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        phrases.update(tokens)
        
        # 2. –î–æ–±–∞–≤–ª—è–µ–º –±–∏–≥—Ä–∞–º–º—ã
        bigrams = self._extract_bigrams(tokens)
        phrases.update(bigrams)
        
        # 3. –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∏–≥—Ä–∞–º–º—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if self.use_trigrams:
            trigrams = self._extract_trigrams(tokens)
            phrases.update(trigrams)
        
        logger.debug(
            f"üîç –ò–∑–≤–ª–µ—á–µ–Ω–æ —Ñ—Ä–∞–∑: {len(phrases)} "
            f"(—Ç–æ–∫–µ–Ω–æ–≤: {len(tokens)}, –±–∏–≥—Ä–∞–º–º: {len(bigrams)})"
        )
        
        return phrases
    
    def _tokenize(self, text: str) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π.
        
        Args:
            text: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        """
        all_tokens = text.split()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã –ø–æ –¥–ª–∏–Ω–µ
        filtered = [
            token for token in all_tokens
            if len(token) >= self.min_token_length
        ]
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        tokens = filtered[:self.max_tokens]
        
        return tokens
    
    def _extract_bigrams(self, tokens: List[str]) -> Set[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –±–∏–≥—Ä–∞–º–º—ã –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤.
        
        Args:
            tokens: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
            
        Returns:
            –ù–∞–±–æ—Ä –±–∏–≥—Ä–∞–º–º
        """
        bigrams = set()
        
        for i in range(len(tokens) - 1):
            bigram = f"{tokens[i]} {tokens[i + 1]}"
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ
            if self.min_phrase_length <= len(bigram) <= self.max_phrase_length:
                bigrams.add(bigram)
        
        return bigrams
    
    def _extract_trigrams(self, tokens: List[str]) -> Set[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç—Ä–∏–≥—Ä–∞–º–º—ã –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤.
        
        Args:
            tokens: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
            
        Returns:
            –ù–∞–±–æ—Ä —Ç—Ä–∏–≥—Ä–∞–º–º
        """
        trigrams = set()
        
        for i in range(len(tokens) - 2):
            trigram = f"{tokens[i]} {tokens[i + 1]} {tokens[i + 2]}"
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ
            if self.min_phrase_length <= len(trigram) <= self.max_phrase_length:
                trigrams.add(trigram)
        
        return trigrams
    
    def extract_with_metadata(self, text: str) -> dict:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ—Ä–∞–∑—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏.
        
        Args:
            text: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ñ—Ä–∞–∑–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        tokens = self._tokenize(text)
        bigrams = self._extract_bigrams(tokens)
        
        metadata = {
            "total_phrases": len(tokens) + len(bigrams),
            "tokens": list(tokens),
            "bigrams": list(bigrams),
            "token_count": len(tokens),
            "bigram_count": len(bigrams),
        }
        
        if self.use_trigrams:
            trigrams = self._extract_trigrams(tokens)
            metadata["trigrams"] = list(trigrams)
            metadata["trigram_count"] = len(trigrams)
            metadata["total_phrases"] += len(trigrams)
        
        return metadata